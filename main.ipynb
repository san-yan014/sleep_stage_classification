{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4370fd-d17c-4082-b9fe-6cfa83cd736f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: torch in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\willi\\onedrive - northeastern university\\personal projects\\sleep study using ihr\\my own project\\sleep_study\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy scikit-learn xgboost pandas torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e9466f-9203-4a52-91d2-9cdfd24591d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.stats import skew\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6acd47-b9e8-42b3-bf48-5299bb663544",
   "metadata": {},
   "source": [
    "#### Seeing how the dataset is structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad766f8-04d0-4522-b0f4-08bc88050723",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1dd9730-87f1-4b42-8243-194a5826cb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>BVP</th>\n",
       "      <th>ACC_X</th>\n",
       "      <th>ACC_Y</th>\n",
       "      <th>ACC_Z</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>HR</th>\n",
       "      <th>IBI</th>\n",
       "      <th>Sleep_Stage</th>\n",
       "      <th>Obstructive_Apnea</th>\n",
       "      <th>Central_Apnea</th>\n",
       "      <th>Hypopnea</th>\n",
       "      <th>Multiple_Events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.14</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.53</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.28</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.53</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>3.51</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.53</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046875</td>\n",
       "      <td>3.02</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.53</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>2.94</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>35.53</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TIMESTAMP   BVP  ACC_X  ACC_Y  ACC_Z   TEMP       EDA    HR  IBI  \\\n",
       "0   0.000000  5.14   31.0    8.0   55.0  35.53  0.073005  49.0  NaN   \n",
       "1   0.015625  4.28   31.0    8.0   55.0  35.53  0.073005  49.0  NaN   \n",
       "2   0.031250  3.51   31.0    8.0   55.0  35.53  0.073005  49.0  NaN   \n",
       "3   0.046875  3.02   31.0    8.0   55.0  35.53  0.073005  49.0  NaN   \n",
       "4   0.062500  2.94   28.0    8.0   55.0  35.53  0.073005  49.0  NaN   \n",
       "\n",
       "  Sleep_Stage  Obstructive_Apnea  Central_Apnea  Hypopnea  Multiple_Events  \n",
       "0           P                NaN            NaN       NaN              NaN  \n",
       "1           P                NaN            NaN       NaN              NaN  \n",
       "2           P                NaN            NaN       NaN              NaN  \n",
       "3           P                NaN            NaN       NaN              NaN  \n",
       "4           P                NaN            NaN       NaN              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_path+\"data_64Hz/S002_whole_df.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f572449-d684-415f-a16c-206cac480171",
   "metadata": {},
   "source": [
    "### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24eb109e-9425-4e66-9dd8-7c754474f655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIMESTAMP                  0\n",
       "BVP                        0\n",
       "ACC_X                      0\n",
       "ACC_Y                      0\n",
       "ACC_Z                      0\n",
       "TEMP                       0\n",
       "EDA                        0\n",
       "HR                         0\n",
       "IBI                     1488\n",
       "Sleep_Stage                0\n",
       "Obstructive_Apnea    2004417\n",
       "Central_Apnea        2013697\n",
       "Hypopnea             1926605\n",
       "Multiple_Events      2013697\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6e5fe-bd78-4e49-ac0d-80f53946f68d",
   "metadata": {},
   "source": [
    "#### None of the columns that I am planning to use having missing values. Then, I will check the type of data of each column to check if further preprocessing is required for the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660e5f3b-4f02-4493-8af6-45df1206e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2013697 entries, 0 to 2013696\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   TIMESTAMP          float64\n",
      " 1   BVP                float64\n",
      " 2   ACC_X              float64\n",
      " 3   ACC_Y              float64\n",
      " 4   ACC_Z              float64\n",
      " 5   TEMP               float64\n",
      " 6   EDA                float64\n",
      " 7   HR                 float64\n",
      " 8   IBI                float64\n",
      " 9   Sleep_Stage        object \n",
      " 10  Obstructive_Apnea  float64\n",
      " 11  Central_Apnea      float64\n",
      " 12  Hypopnea           float64\n",
      " 13  Multiple_Events    float64\n",
      "dtypes: float64(13), object(1)\n",
      "memory usage: 215.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b014d-7b74-40b6-8c07-a394ca937eae",
   "metadata": {},
   "source": [
    "#### After reading the notes for each column, every column is in the right data type. I will now check to see what values are in Sleep_stage since that is the target column. I also want to ensure strings like \"missing\", \"no data\", etc are not being recorded in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf7789f-434a-4220-99d3-3fc93dc58e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sleep_Stage\n",
       "N2    641280\n",
       "P     587136\n",
       "W     508801\n",
       "R     153600\n",
       "N1    122880\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sleep_Stage\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e8deb-3568-47cf-8024-90f2aa5e3df3",
   "metadata": {},
   "source": [
    "#### One of the files of the data is pretty reliable and are all in the correct format. I am going to assume that every other file is formatted the same way. However, I want to confirm if Sleep_stage and enough data exists for other patients' files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080c3ea8-89fc-41f9-8947-d020969c8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_participants(data_path):\n",
    "    \"\"\"\n",
    "    Analyze and visualize all participant files without processing data\n",
    "    \n",
    "    Returns:\n",
    "    participant_info: dict with analysis results for each participant\n",
    "    \"\"\"\n",
    "    \n",
    "    folder_path = os.path.join(data_path, \"data_64Hz\")\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    participant_info = {}\n",
    "    \n",
    "    # Initialize aggregation variables\n",
    "    total_missing_data = {}\n",
    "    total_sleep_stages = {}\n",
    "    total_rows = 0\n",
    "    total_sensor_data_points = 0\n",
    "    total_missing_sensor_points = 0\n",
    "    sensor_cols = ['BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR', 'IBI']\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PARTICIPANT DATA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processing {len(csv_files)} participant files...\")\n",
    "    \n",
    "    for i, filename in enumerate(sorted(csv_files)):\n",
    "        print(f\"Processing {filename}... ({i+1}/{len(csv_files)})\")\n",
    "        \n",
    "        # Read the file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Aggregate basic info\n",
    "        total_rows += len(df)\n",
    "        \n",
    "        # Aggregate missing values\n",
    "        missing_data = df.isnull().sum()\n",
    "        for col, missing_count in missing_data.items():\n",
    "            if col not in total_missing_data:\n",
    "                total_missing_data[col] = 0\n",
    "            total_missing_data[col] += missing_count\n",
    "        \n",
    "        # Aggregate sleep stage counts\n",
    "        sleep_stages = df[\"Sleep_Stage\"].value_counts()\n",
    "        for stage, count in sleep_stages.items():\n",
    "            if stage not in total_sleep_stages:\n",
    "                total_sleep_stages[stage] = 0\n",
    "            total_sleep_stages[stage] += count\n",
    "        \n",
    "        # Aggregate sensor data points for quality calculation\n",
    "        participant_sensor_points = len(df) * len(sensor_cols)\n",
    "        participant_missing_sensor = missing_data[sensor_cols].sum()\n",
    "        \n",
    "        total_sensor_data_points += participant_sensor_points\n",
    "        total_missing_sensor_points += participant_missing_sensor\n",
    "        \n",
    "        # Calculate individual data quality\n",
    "        individual_quality = 1 - (participant_missing_sensor / participant_sensor_points)\n",
    "        \n",
    "        # Store participant info\n",
    "        participant_info[filename] = {\n",
    "            'shape': df.shape,\n",
    "            'missing_data': missing_data.to_dict(),\n",
    "            'sleep_stages': sleep_stages.to_dict(),\n",
    "            'data_quality': individual_quality\n",
    "        }\n",
    "    \n",
    "    # Calculate overall data quality\n",
    "    overall_data_quality = 1 - (total_missing_sensor_points / total_sensor_data_points)\n",
    "    \n",
    "    # Display aggregated results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATED RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total participants analyzed: {len(csv_files)}\")\n",
    "    print(f\"Total rows across all files: {total_rows:,}\")\n",
    "    \n",
    "    print(f\"\\nTotal missing values (percentage of all data):\")\n",
    "    for col, missing_count in total_missing_data.items():\n",
    "        total_col_data = total_rows  # Each column has total_rows data points\n",
    "        missing_pct = (missing_count / total_col_data) * 100\n",
    "        print(f\"  {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal sleep stage counts:\")\n",
    "    total_sleep_observations = sum(total_sleep_stages.values())\n",
    "    for stage, count in sorted(total_sleep_stages.items()):\n",
    "        percentage = (count / total_sleep_observations) * 100\n",
    "        print(f\"  {stage}: {count:,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nOverall data quality score: {overall_data_quality:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef80852-b8db-43dc-9c84-b05222bc4ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARTICIPANT DATA ANALYSIS\n",
      "============================================================\n",
      "Processing 100 participant files...\n",
      "Processing S002_whole_df.csv... (1/100)\n",
      "Processing S003_whole_df.csv... (2/100)\n",
      "Processing S004_whole_df.csv... (3/100)\n",
      "Processing S005_whole_df.csv... (4/100)\n",
      "Processing S006_whole_df.csv... (5/100)\n",
      "Processing S007_whole_df.csv... (6/100)\n",
      "Processing S008_whole_df.csv... (7/100)\n",
      "Processing S009_whole_df.csv... (8/100)\n",
      "Processing S010_whole_df.csv... (9/100)\n",
      "Processing S011_whole_df.csv... (10/100)\n",
      "Processing S012_whole_df.csv... (11/100)\n",
      "Processing S013_whole_df.csv... (12/100)\n",
      "Processing S014_whole_df.csv... (13/100)\n",
      "Processing S015_whole_df.csv... (14/100)\n",
      "Processing S016_whole_df.csv... (15/100)\n",
      "Processing S017_whole_df.csv... (16/100)\n",
      "Processing S018_whole_df.csv... (17/100)\n",
      "Processing S019_whole_df.csv... (18/100)\n",
      "Processing S020_whole_df.csv... (19/100)\n",
      "Processing S021_whole_df.csv... (20/100)\n",
      "Processing S022_whole_df.csv... (21/100)\n",
      "Processing S023_whole_df.csv... (22/100)\n",
      "Processing S024_whole_df.csv... (23/100)\n",
      "Processing S025_whole_df.csv... (24/100)\n",
      "Processing S026_whole_df.csv... (25/100)\n",
      "Processing S027_whole_df.csv... (26/100)\n",
      "Processing S028_whole_df.csv... (27/100)\n",
      "Processing S029_whole_df.csv... (28/100)\n",
      "Processing S030_whole_df.csv... (29/100)\n",
      "Processing S031_whole_df.csv... (30/100)\n",
      "Processing S032_whole_df.csv... (31/100)\n",
      "Processing S033_whole_df.csv... (32/100)\n",
      "Processing S034_whole_df.csv... (33/100)\n",
      "Processing S035_whole_df.csv... (34/100)\n",
      "Processing S036_whole_df.csv... (35/100)\n",
      "Processing S037_whole_df.csv... (36/100)\n",
      "Processing S038_whole_df.csv... (37/100)\n",
      "Processing S039_whole_df.csv... (38/100)\n",
      "Processing S040_whole_df.csv... (39/100)\n",
      "Processing S042_whole_df.csv... (40/100)\n",
      "Processing S043_whole_df.csv... (41/100)\n",
      "Processing S044_whole_df.csv... (42/100)\n",
      "Processing S045_whole_df.csv... (43/100)\n",
      "Processing S046_whole_df.csv... (44/100)\n",
      "Processing S047_whole_df.csv... (45/100)\n",
      "Processing S048_whole_df.csv... (46/100)\n",
      "Processing S049_whole_df.csv... (47/100)\n",
      "Processing S050_whole_df.csv... (48/100)\n",
      "Processing S051_whole_df.csv... (49/100)\n",
      "Processing S052_whole_df.csv... (50/100)\n",
      "Processing S053_whole_df.csv... (51/100)\n",
      "Processing S054_whole_df.csv... (52/100)\n",
      "Processing S055_whole_df.csv... (53/100)\n",
      "Processing S056_whole_df.csv... (54/100)\n",
      "Processing S057_whole_df.csv... (55/100)\n",
      "Processing S058_whole_df.csv... (56/100)\n",
      "Processing S059_whole_df.csv... (57/100)\n",
      "Processing S061_whole_df.csv... (58/100)\n",
      "Processing S062_whole_df.csv... (59/100)\n",
      "Processing S063_whole_df.csv... (60/100)\n",
      "Processing S064_whole_df.csv... (61/100)\n",
      "Processing S065_whole_df.csv... (62/100)\n",
      "Processing S066_whole_df.csv... (63/100)\n",
      "Processing S067_whole_df.csv... (64/100)\n",
      "Processing S068_whole_df.csv... (65/100)\n",
      "Processing S069_whole_df.csv... (66/100)\n",
      "Processing S070_whole_df.csv... (67/100)\n",
      "Processing S071_whole_df.csv... (68/100)\n",
      "Processing S072_whole_df.csv... (69/100)\n",
      "Processing S073_whole_df.csv... (70/100)\n",
      "Processing S074_whole_df.csv... (71/100)\n",
      "Processing S075_whole_df.csv... (72/100)\n",
      "Processing S076_whole_df.csv... (73/100)\n",
      "Processing S077_whole_df.csv... (74/100)\n",
      "Processing S078_whole_df.csv... (75/100)\n",
      "Processing S079_whole_df.csv... (76/100)\n",
      "Processing S080_whole_df.csv... (77/100)\n",
      "Processing S081_whole_df.csv... (78/100)\n",
      "Processing S082_whole_df.csv... (79/100)\n",
      "Processing S083_whole_df.csv... (80/100)\n",
      "Processing S084_whole_df.csv... (81/100)\n",
      "Processing S085_whole_df.csv... (82/100)\n",
      "Processing S086_whole_df.csv... (83/100)\n",
      "Processing S087_whole_df.csv... (84/100)\n",
      "Processing S088_whole_df.csv... (85/100)\n",
      "Processing S089_whole_df.csv... (86/100)\n",
      "Processing S090_whole_df.csv... (87/100)\n",
      "Processing S091_whole_df.csv... (88/100)\n",
      "Processing S092_whole_df.csv... (89/100)\n",
      "Processing S093_whole_df.csv... (90/100)\n",
      "Processing S094_whole_df.csv... (91/100)\n",
      "Processing S095_whole_df.csv... (92/100)\n",
      "Processing S096_whole_df.csv... (93/100)\n",
      "Processing S097_whole_df.csv... (94/100)\n",
      "Processing S098_whole_df.csv... (95/100)\n",
      "Processing S099_whole_df.csv... (96/100)\n",
      "Processing S100_whole_df.csv... (97/100)\n",
      "Processing S101_whole_df.csv... (98/100)\n",
      "Processing S102_whole_df.csv... (99/100)\n",
      "Processing S103_whole_df.csv... (100/100)\n",
      "\n",
      "============================================================\n",
      "AGGREGATED RESULTS\n",
      "============================================================\n",
      "Total participants analyzed: 100\n",
      "Total rows across all files: 203,531,428\n",
      "\n",
      "Total missing values (percentage of all data):\n",
      "  TIMESTAMP: 0 (0.00%)\n",
      "  BVP: 0 (0.00%)\n",
      "  ACC_X: 0 (0.00%)\n",
      "  ACC_Y: 0 (0.00%)\n",
      "  ACC_Z: 0 (0.00%)\n",
      "  TEMP: 0 (0.00%)\n",
      "  EDA: 0 (0.00%)\n",
      "  HR: 0 (0.00%)\n",
      "  IBI: 1,004,340 (0.49%)\n",
      "  Sleep_Stage: 0 (0.00%)\n",
      "  Obstructive_Apnea: 201,158,891 (98.83%)\n",
      "  Central_Apnea: 202,187,222 (99.34%)\n",
      "  Hypopnea: 191,715,892 (94.19%)\n",
      "  Multiple_Events: 203,160,492 (99.82%)\n",
      "\n",
      "Total sleep stage counts:\n",
      "  Missing: 120,960 (0.06%)\n",
      "  N1: 16,965,129 (8.34%)\n",
      "  N2: 76,978,529 (37.82%)\n",
      "  N3: 5,191,680 (2.55%)\n",
      "  P: 49,443,712 (24.29%)\n",
      "  R: 16,152,968 (7.94%)\n",
      "  W: 38,678,450 (19.00%)\n",
      "\n",
      "Overall data quality score: 0.999\n"
     ]
    }
   ],
   "source": [
    "visualize_all_participants(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530dcefa-8a55-469f-8759-02c518038ae7",
   "metadata": {},
   "source": [
    "#### There seems to be \"Missing\" string values in some patients' files in the Sleep_stage. Even though we have a a surplus of data windows, I still want to preserve data. My method of dealing with this is to fill out the missing windows with the sleep stage of the window before. I am assuming that the researchers who recorded the data did not record for this time frame because there were no changes to the patient and decided not to record. However, I did leave an alternative function to deal with this matter in case I wanted a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55462bb2-ac47-49a5-86f5-dde8152bff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_missing_windows(df):\n",
    "    \"\"\"\n",
    "    Remove entire 30-second windows (1920 rows) that contain 'Missing' sleep stages\n",
    "    \"\"\"\n",
    "    clean_windows = []\n",
    "    \n",
    "    for i in range(0, len(df), 1920):  # Every 1920 rows\n",
    "        window = df.iloc[i:i+1920]\n",
    "        \n",
    "        # Check if this window has \"Missing\" sleep stage\n",
    "        if 'Missing' not in window['Sleep_Stage'].values:\n",
    "            clean_windows.append(window)\n",
    "        else:\n",
    "            print(f\"Dropping window starting at row {i} due to 'Missing' sleep stage\")\n",
    "    \n",
    "    if clean_windows:\n",
    "        return pd.concat(clean_windows, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no clean windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d6ca5d-e3d2-48d4-ae35-453a473f4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_participants_to_arrays(data_path):\n",
    "    \"\"\"\n",
    "    Process all participant files and return 3D arrays for training\n",
    "    \n",
    "    Returns:\n",
    "    all_windowed_data: numpy array of shape (total_windows, 1920, 8) - all sensor data\n",
    "    all_labels: numpy array of sleep stage labels for each window\n",
    "    \"\"\"\n",
    "    \n",
    "    folder_path = os.path.join(data_path, \"data_64Hz\")\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    all_windowed_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PROCESSING PARTICIPANTS TO 3D ARRAYS\")\n",
    "    print(\"=\"*60)\n",
    "       \n",
    "    for i, filename in enumerate(sorted(csv_files)):\n",
    "        print(f\"\\n--- PROCESSING PARTICIPANT {i+1}: {filename} ---\")\n",
    "        \n",
    "        # Read the file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # modifying the \"P\" stage as \"W\" since patient is technically awake\n",
    "        df[\"Sleep_Stage\"] = df[\"Sleep_Stage\"].str.replace(\"P\", \"W\")\n",
    "        \n",
    "        # Fill missing IBI values with 0 since most of the missing values are at the beginning - smooth out potential middle gaps with \n",
    "        # interpolation\n",
    "        # the missing values at the beginning rows where interfering with the statistics that were needed for the basline model, XGBoost. \n",
    "        df['IBI'] = df['IBI'].fillna(0).interpolate(method='linear')\n",
    "\n",
    "        \n",
    "        # deal with windows with \"Missing\" sleep stages\n",
    "        df_clean = deal_missing_windows(df)\n",
    "\n",
    "        if len(df_clean) > 0:\n",
    "            # Create windows\n",
    "            n_complete_windows = len(df_clean) // 1920\n",
    "            df_trimmed = df_clean.iloc[:n_complete_windows * 1920]\n",
    "            \n",
    "            # Extract sensor data\n",
    "            sensor_cols = ['BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR', 'IBI']\n",
    "            sensor_data = df_trimmed[sensor_cols].values\n",
    "            windowed_data = sensor_data.reshape(n_complete_windows, 1920, len(sensor_cols))\n",
    "            \n",
    "            # Extract labels\n",
    "            labels = df_trimmed['Sleep_Stage'].iloc[::1920].values\n",
    "            \n",
    "            # Add to collections\n",
    "            all_windowed_data.append(windowed_data)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            print(f\"  Windows created: {n_complete_windows}\")\n",
    "        else:\n",
    "            print(f\"  No clean windows available\")\n",
    "\n",
    "    # Combine all participants\n",
    "    if all_windowed_data:\n",
    "        all_windowed_data = np.concatenate(all_windowed_data, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL 3D DATASET\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total windows: {len(all_windowed_data)}\")\n",
    "        print(f\"Data shape: {all_windowed_data.shape}\")\n",
    "        print(f\"Labels shape: {all_labels.shape}\")\n",
    "        print(f\"Unique sleep stages: {np.unique(all_labels)}\")\n",
    "        \n",
    "        return all_windowed_data, all_labels\n",
    "    else:\n",
    "        print(\"No usable data found!\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09255aa3-911a-46c2-b5a7-ab859dd90df0",
   "metadata": {},
   "source": [
    "### To create a flexible and effective dataset, I reformatted the panda dataframes into a 3D numpy array (# of total windows, 1920 - # of rows in a window, 8 - # of features). With this way of formatting, I can easily train my models and easily access the data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f406272-417b-4103-b1fa-2c4be1cce457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESSING PARTICIPANTS TO 3D ARRAYS\n",
      "============================================================\n",
      "\n",
      "--- PROCESSING PARTICIPANT 1: S002_whole_df.csv ---\n",
      "  Windows created: 1048\n",
      "\n",
      "--- PROCESSING PARTICIPANT 2: S003_whole_df.csv ---\n",
      "  Windows created: 1068\n",
      "\n",
      "--- PROCESSING PARTICIPANT 3: S004_whole_df.csv ---\n",
      "  Windows created: 1032\n",
      "\n",
      "--- PROCESSING PARTICIPANT 4: S005_whole_df.csv ---\n",
      "  Windows created: 1037\n",
      "\n",
      "--- PROCESSING PARTICIPANT 5: S006_whole_df.csv ---\n",
      "  Windows created: 1113\n",
      "\n",
      "--- PROCESSING PARTICIPANT 6: S007_whole_df.csv ---\n",
      "  Windows created: 1024\n",
      "\n",
      "--- PROCESSING PARTICIPANT 7: S008_whole_df.csv ---\n",
      "  Windows created: 1064\n",
      "\n",
      "--- PROCESSING PARTICIPANT 8: S009_whole_df.csv ---\n",
      "  Windows created: 1025\n",
      "\n",
      "--- PROCESSING PARTICIPANT 9: S010_whole_df.csv ---\n",
      "  Windows created: 1129\n",
      "\n",
      "--- PROCESSING PARTICIPANT 10: S011_whole_df.csv ---\n",
      "  Windows created: 1067\n",
      "\n",
      "--- PROCESSING PARTICIPANT 11: S012_whole_df.csv ---\n",
      "  Windows created: 1168\n",
      "\n",
      "--- PROCESSING PARTICIPANT 12: S013_whole_df.csv ---\n",
      "  Windows created: 1070\n",
      "\n",
      "--- PROCESSING PARTICIPANT 13: S014_whole_df.csv ---\n",
      "  Windows created: 954\n",
      "\n",
      "--- PROCESSING PARTICIPANT 14: S015_whole_df.csv ---\n",
      "  Windows created: 1236\n",
      "\n",
      "--- PROCESSING PARTICIPANT 15: S016_whole_df.csv ---\n",
      "  Windows created: 1070\n",
      "\n",
      "--- PROCESSING PARTICIPANT 16: S017_whole_df.csv ---\n",
      "  Windows created: 1125\n",
      "\n",
      "--- PROCESSING PARTICIPANT 17: S018_whole_df.csv ---\n",
      "  Windows created: 1053\n",
      "\n",
      "--- PROCESSING PARTICIPANT 18: S019_whole_df.csv ---\n",
      "  Windows created: 983\n",
      "\n",
      "--- PROCESSING PARTICIPANT 19: S020_whole_df.csv ---\n",
      "  Windows created: 968\n",
      "\n",
      "--- PROCESSING PARTICIPANT 20: S021_whole_df.csv ---\n",
      "  Windows created: 1077\n",
      "\n",
      "--- PROCESSING PARTICIPANT 21: S022_whole_df.csv ---\n",
      "  Windows created: 1103\n",
      "\n",
      "--- PROCESSING PARTICIPANT 22: S023_whole_df.csv ---\n",
      "  Windows created: 1061\n",
      "\n",
      "--- PROCESSING PARTICIPANT 23: S024_whole_df.csv ---\n",
      "  Windows created: 1129\n",
      "\n",
      "--- PROCESSING PARTICIPANT 24: S025_whole_df.csv ---\n",
      "  Windows created: 1037\n",
      "\n",
      "--- PROCESSING PARTICIPANT 25: S026_whole_df.csv ---\n",
      "  Windows created: 1038\n",
      "\n",
      "--- PROCESSING PARTICIPANT 26: S027_whole_df.csv ---\n",
      "Dropping window starting at row 1107840 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1109760 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1111680 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1113600 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1115520 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1117440 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1119360 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1121280 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1123200 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1125120 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1127040 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1128960 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1130880 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1132800 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1134720 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1136640 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1138560 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1140480 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1142400 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1144320 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1146240 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1148160 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1150080 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1152000 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1153920 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1155840 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1157760 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1159680 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1161600 due to 'Missing' sleep stage\n",
      "  Windows created: 1126\n",
      "\n",
      "--- PROCESSING PARTICIPANT 27: S028_whole_df.csv ---\n",
      "  Windows created: 1146\n",
      "\n",
      "--- PROCESSING PARTICIPANT 28: S029_whole_df.csv ---\n",
      "  Windows created: 1089\n",
      "\n",
      "--- PROCESSING PARTICIPANT 29: S030_whole_df.csv ---\n",
      "  Windows created: 1063\n",
      "\n",
      "--- PROCESSING PARTICIPANT 30: S031_whole_df.csv ---\n",
      "  Windows created: 1119\n",
      "\n",
      "--- PROCESSING PARTICIPANT 31: S032_whole_df.csv ---\n",
      "  Windows created: 1025\n",
      "\n",
      "--- PROCESSING PARTICIPANT 32: S033_whole_df.csv ---\n",
      "  Windows created: 1019\n",
      "\n",
      "--- PROCESSING PARTICIPANT 33: S034_whole_df.csv ---\n",
      "  Windows created: 871\n",
      "\n",
      "--- PROCESSING PARTICIPANT 34: S035_whole_df.csv ---\n",
      "  Windows created: 1070\n",
      "\n",
      "--- PROCESSING PARTICIPANT 35: S036_whole_df.csv ---\n",
      "  Windows created: 1234\n",
      "\n",
      "--- PROCESSING PARTICIPANT 36: S037_whole_df.csv ---\n",
      "  Windows created: 1115\n",
      "\n",
      "--- PROCESSING PARTICIPANT 37: S038_whole_df.csv ---\n",
      "  Windows created: 1038\n",
      "\n",
      "--- PROCESSING PARTICIPANT 38: S039_whole_df.csv ---\n",
      "  Windows created: 1029\n",
      "\n",
      "--- PROCESSING PARTICIPANT 39: S040_whole_df.csv ---\n",
      "  Windows created: 1085\n",
      "\n",
      "--- PROCESSING PARTICIPANT 40: S042_whole_df.csv ---\n",
      "  Windows created: 1060\n",
      "\n",
      "--- PROCESSING PARTICIPANT 41: S043_whole_df.csv ---\n",
      "  Windows created: 1078\n",
      "\n",
      "--- PROCESSING PARTICIPANT 42: S044_whole_df.csv ---\n",
      "  Windows created: 994\n",
      "\n",
      "--- PROCESSING PARTICIPANT 43: S045_whole_df.csv ---\n",
      "  Windows created: 1001\n",
      "\n",
      "--- PROCESSING PARTICIPANT 44: S046_whole_df.csv ---\n",
      "  Windows created: 1067\n",
      "\n",
      "--- PROCESSING PARTICIPANT 45: S047_whole_df.csv ---\n",
      "  Windows created: 1044\n",
      "\n",
      "--- PROCESSING PARTICIPANT 46: S048_whole_df.csv ---\n",
      "  Windows created: 1096\n",
      "\n",
      "--- PROCESSING PARTICIPANT 47: S049_whole_df.csv ---\n",
      "  Windows created: 1116\n",
      "\n",
      "--- PROCESSING PARTICIPANT 48: S050_whole_df.csv ---\n",
      "  Windows created: 951\n",
      "\n",
      "--- PROCESSING PARTICIPANT 49: S051_whole_df.csv ---\n",
      "  Windows created: 1056\n",
      "\n",
      "--- PROCESSING PARTICIPANT 50: S052_whole_df.csv ---\n",
      "  Windows created: 1070\n",
      "\n",
      "--- PROCESSING PARTICIPANT 51: S053_whole_df.csv ---\n",
      "  Windows created: 999\n",
      "\n",
      "--- PROCESSING PARTICIPANT 52: S054_whole_df.csv ---\n",
      "  Windows created: 1101\n",
      "\n",
      "--- PROCESSING PARTICIPANT 53: S055_whole_df.csv ---\n",
      "  Windows created: 1016\n",
      "\n",
      "--- PROCESSING PARTICIPANT 54: S056_whole_df.csv ---\n",
      "  Windows created: 1018\n",
      "\n",
      "--- PROCESSING PARTICIPANT 55: S057_whole_df.csv ---\n",
      "  Windows created: 1082\n",
      "\n",
      "--- PROCESSING PARTICIPANT 56: S058_whole_df.csv ---\n",
      "  Windows created: 1050\n",
      "\n",
      "--- PROCESSING PARTICIPANT 57: S059_whole_df.csv ---\n",
      "  Windows created: 1039\n",
      "\n",
      "--- PROCESSING PARTICIPANT 58: S061_whole_df.csv ---\n",
      "  Windows created: 1100\n",
      "\n",
      "--- PROCESSING PARTICIPANT 59: S062_whole_df.csv ---\n",
      "  Windows created: 1049\n",
      "\n",
      "--- PROCESSING PARTICIPANT 60: S063_whole_df.csv ---\n",
      "  Windows created: 989\n",
      "\n",
      "--- PROCESSING PARTICIPANT 61: S064_whole_df.csv ---\n",
      "  Windows created: 1125\n",
      "\n",
      "--- PROCESSING PARTICIPANT 62: S065_whole_df.csv ---\n",
      "  Windows created: 1073\n",
      "\n",
      "--- PROCESSING PARTICIPANT 63: S066_whole_df.csv ---\n",
      "  Windows created: 1063\n",
      "\n",
      "--- PROCESSING PARTICIPANT 64: S067_whole_df.csv ---\n",
      "  Windows created: 1077\n",
      "\n",
      "--- PROCESSING PARTICIPANT 65: S068_whole_df.csv ---\n",
      "  Windows created: 1031\n",
      "\n",
      "--- PROCESSING PARTICIPANT 66: S069_whole_df.csv ---\n",
      "  Windows created: 1152\n",
      "\n",
      "--- PROCESSING PARTICIPANT 67: S070_whole_df.csv ---\n",
      "  Windows created: 1119\n",
      "\n",
      "--- PROCESSING PARTICIPANT 68: S071_whole_df.csv ---\n",
      "  Windows created: 1069\n",
      "\n",
      "--- PROCESSING PARTICIPANT 69: S072_whole_df.csv ---\n",
      "  Windows created: 1043\n",
      "\n",
      "--- PROCESSING PARTICIPANT 70: S073_whole_df.csv ---\n",
      "  Windows created: 1067\n",
      "\n",
      "--- PROCESSING PARTICIPANT 71: S074_whole_df.csv ---\n",
      "  Windows created: 1086\n",
      "\n",
      "--- PROCESSING PARTICIPANT 72: S075_whole_df.csv ---\n",
      "  Windows created: 1032\n",
      "\n",
      "--- PROCESSING PARTICIPANT 73: S076_whole_df.csv ---\n",
      "  Windows created: 1035\n",
      "\n",
      "--- PROCESSING PARTICIPANT 74: S077_whole_df.csv ---\n",
      "Dropping window starting at row 1351680 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1353600 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1355520 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1357440 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1359360 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1361280 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1363200 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1365120 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1367040 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1368960 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1370880 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1372800 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1374720 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1376640 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1378560 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1380480 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1382400 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1384320 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1386240 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1388160 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1390080 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1392000 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1393920 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1395840 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1397760 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1399680 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1401600 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1403520 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1405440 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1407360 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1409280 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1411200 due to 'Missing' sleep stage\n",
      "  Windows created: 1128\n",
      "\n",
      "--- PROCESSING PARTICIPANT 75: S078_whole_df.csv ---\n",
      "  Windows created: 1079\n",
      "\n",
      "--- PROCESSING PARTICIPANT 76: S079_whole_df.csv ---\n",
      "  Windows created: 1073\n",
      "\n",
      "--- PROCESSING PARTICIPANT 77: S080_whole_df.csv ---\n",
      "  Windows created: 1066\n",
      "\n",
      "--- PROCESSING PARTICIPANT 78: S081_whole_df.csv ---\n",
      "  Windows created: 953\n",
      "\n",
      "--- PROCESSING PARTICIPANT 79: S082_whole_df.csv ---\n",
      "  Windows created: 1046\n",
      "\n",
      "--- PROCESSING PARTICIPANT 80: S083_whole_df.csv ---\n",
      "  Windows created: 967\n",
      "\n",
      "--- PROCESSING PARTICIPANT 81: S084_whole_df.csv ---\n",
      "  Windows created: 1057\n",
      "\n",
      "--- PROCESSING PARTICIPANT 82: S085_whole_df.csv ---\n",
      "  Windows created: 1015\n",
      "\n",
      "--- PROCESSING PARTICIPANT 83: S086_whole_df.csv ---\n",
      "  Windows created: 1067\n",
      "\n",
      "--- PROCESSING PARTICIPANT 84: S087_whole_df.csv ---\n",
      "  Windows created: 982\n",
      "\n",
      "--- PROCESSING PARTICIPANT 85: S088_whole_df.csv ---\n",
      "  Windows created: 1110\n",
      "\n",
      "--- PROCESSING PARTICIPANT 86: S089_whole_df.csv ---\n",
      "Dropping window starting at row 1879680 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1881600 due to 'Missing' sleep stage\n",
      "  Windows created: 1059\n",
      "\n",
      "--- PROCESSING PARTICIPANT 87: S090_whole_df.csv ---\n",
      "  Windows created: 1048\n",
      "\n",
      "--- PROCESSING PARTICIPANT 88: S091_whole_df.csv ---\n",
      "  Windows created: 1088\n",
      "\n",
      "--- PROCESSING PARTICIPANT 89: S092_whole_df.csv ---\n",
      "  Windows created: 1037\n",
      "\n",
      "--- PROCESSING PARTICIPANT 90: S093_whole_df.csv ---\n",
      "Dropping window starting at row 1770240 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1772160 due to 'Missing' sleep stage\n",
      "  Windows created: 1023\n",
      "\n",
      "--- PROCESSING PARTICIPANT 91: S094_whole_df.csv ---\n",
      "  Windows created: 1057\n",
      "\n",
      "--- PROCESSING PARTICIPANT 92: S095_whole_df.csv ---\n",
      "Dropping window starting at row 1338240 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1340160 due to 'Missing' sleep stage\n",
      "  Windows created: 1046\n",
      "\n",
      "--- PROCESSING PARTICIPANT 93: S096_whole_df.csv ---\n",
      "Dropping window starting at row 1708800 due to 'Missing' sleep stage\n",
      "Dropping window starting at row 1710720 due to 'Missing' sleep stage\n",
      "  Windows created: 1066\n",
      "\n",
      "--- PROCESSING PARTICIPANT 94: S097_whole_df.csv ---\n",
      "  Windows created: 1002\n",
      "\n",
      "--- PROCESSING PARTICIPANT 95: S098_whole_df.csv ---\n",
      "  Windows created: 1076\n",
      "\n",
      "--- PROCESSING PARTICIPANT 96: S099_whole_df.csv ---\n",
      "  Windows created: 991\n",
      "\n",
      "--- PROCESSING PARTICIPANT 97: S100_whole_df.csv ---\n",
      "  Windows created: 1068\n",
      "\n",
      "--- PROCESSING PARTICIPANT 98: S101_whole_df.csv ---\n",
      "  Windows created: 1093\n",
      "\n",
      "--- PROCESSING PARTICIPANT 99: S102_whole_df.csv ---\n",
      "  Windows created: 1023\n",
      "\n",
      "--- PROCESSING PARTICIPANT 100: S103_whole_df.csv ---\n",
      "  Windows created: 1015\n",
      "\n",
      "============================================================\n",
      "FINAL 3D DATASET\n",
      "============================================================\n",
      "Total windows: 105891\n",
      "Data shape: (105891, 1920, 8)\n",
      "Labels shape: (105891,)\n",
      "Unique sleep stages: ['N1' 'N2' 'N3' 'R' 'W']\n"
     ]
    }
   ],
   "source": [
    "data, labels = process_participants_to_arrays(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1d517-aff9-4cb8-8cc6-1f6d987ae67c",
   "metadata": {},
   "source": [
    "#### Building XGBBoost as my baseline model to compare my main model with. In this case, I chose the XGBoost model for several reasons. \n",
    "    1. I want to familiarize myself with more machine learning practices such as creating baseline model to compare my main model with. \n",
    "    2. I want to compare my CNN model with a model that is a suitable alternative to the dataset so I can evaulate the performance better. \n",
    "    3. XGBoost looks interesting and it is sort of an extension of decision trees - something that I am familiar with. However, I also wish to learn more machine learning models. It utilizes decisions trees but focuses on fixing the errors from previous trees in future trees. Works extremely well with tabular data and our dataset is perfect for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757491fe-779b-4023-92a3-24ca0b33f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_window(window_data):\n",
    "    \"\"\"\n",
    "    Extract statistics from each window frame and return it as an array of features. \n",
    "    \n",
    "    window_data: shape(1920, 8) - one 30-second window\n",
    "    returns: 1D array of statistical features from the window frame\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for sensor in range(window_data.shape[1]): \n",
    "        # Accessing each sensor and computing the statistics for each feature \n",
    "        data = window_data[:, sensor]\n",
    "\n",
    "        # extend varies from append. extend adds to the list iteratively\n",
    "        features.extend([\n",
    "            np.mean(data),              # average \n",
    "            np.max(data),            # maximum\n",
    "            np.min(data),           # minimum\n",
    "            np.percentile(data, 25),    # 2nd quantile\n",
    "            np.percentile(data, 75),   # 3rd quantile\n",
    "            np.ptp(data),     # range\n",
    "            np.var(data),     # variance\n",
    "            np.std(data)      # standard deviation\n",
    "        ])\n",
    "\n",
    "\n",
    "        # there is potential to add more features here for feature specific statistics\n",
    "        \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fc55f9-db01-4b47-99f1-3799de819fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataset(windowed_data):\n",
    "    \"\"\"\n",
    "    Extracting statistics from 3D array to 2D array.  \n",
    "    \n",
    "    Args:\n",
    "        windowed_data (array): 3D data of all windows with features \n",
    "\n",
    "    Returns:\n",
    "        array: 2D array \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, window in enumerate(windowed_data): \n",
    "        stats = extract_features_from_window(window)\n",
    "        data.append(stats)\n",
    "        if (i+1) % 1000 == 0: \n",
    "            print(f\"Processed {i+1}/{len(windowed_data)} windows\")\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0ee2c-623b-4ef8-8a3f-b7723c12c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_features, y): \n",
    "    \"\"\"\n",
    "    Training xgboost model using the features extracted from windows and labels as target.  \n",
    "\n",
    "    Returns model evaluations from the XGBoost. Evaluation method includes accuracy and F1 scores. \n",
    "    \"\"\"    \n",
    "    # converting to labels \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # splitting up data for training. ensuring stratification with sleep stages to keep class distribution across train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, y_encoded, test_size = 0.2, random_state = 42, stratify = y_encoded\n",
    "    )  \n",
    "\n",
    "    # scaling \n",
    "    scaler = StandardScaler() \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # training parameters for XGBoost \n",
    "    model = xgb.XGBClassifier(\n",
    "    objective= 'multi:softmax',   # multiple stages classification  \n",
    "    eval_metric= 'mlogloss',           # picked mlogloss to showcase the model's 'confidence' of prediction for each window  \n",
    "    n_estimators= 150,            # how many epochs of training\n",
    "    learning_rate= 0.1,          # how much each tree contributes to final prediciton \n",
    "    max_depth= 7,               # how deep each tree can grow \n",
    "    subsample= 0.8,            # % of data used in training\n",
    "    colsample_bytree= 0.8     # % of features used in training \n",
    "    )\n",
    "    # training the model \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # making predictions \n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # evaluation \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    f1_macro = f1_score(y_test, y_test_pred, average = \"macro\")\n",
    "    f1_weighted = f1_score(y_test, y_test_pred, average = \"weighted\")\n",
    "\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d82533-3139-414e-baad-6f7ace4699d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/105891 windows\n",
      "Processed 2000/105891 windows\n",
      "Processed 3000/105891 windows\n",
      "Processed 4000/105891 windows\n",
      "Processed 5000/105891 windows\n",
      "Processed 6000/105891 windows\n",
      "Processed 7000/105891 windows\n",
      "Processed 8000/105891 windows\n",
      "Processed 9000/105891 windows\n",
      "Processed 10000/105891 windows\n",
      "Processed 11000/105891 windows\n",
      "Processed 12000/105891 windows\n",
      "Processed 13000/105891 windows\n",
      "Processed 14000/105891 windows\n",
      "Processed 15000/105891 windows\n",
      "Processed 16000/105891 windows\n",
      "Processed 17000/105891 windows\n",
      "Processed 18000/105891 windows\n",
      "Processed 19000/105891 windows\n",
      "Processed 20000/105891 windows\n",
      "Processed 21000/105891 windows\n",
      "Processed 22000/105891 windows\n",
      "Processed 23000/105891 windows\n",
      "Processed 24000/105891 windows\n",
      "Processed 25000/105891 windows\n",
      "Processed 26000/105891 windows\n",
      "Processed 27000/105891 windows\n",
      "Processed 28000/105891 windows\n",
      "Processed 29000/105891 windows\n",
      "Processed 30000/105891 windows\n",
      "Processed 31000/105891 windows\n",
      "Processed 32000/105891 windows\n",
      "Processed 33000/105891 windows\n",
      "Processed 34000/105891 windows\n",
      "Processed 35000/105891 windows\n",
      "Processed 36000/105891 windows\n",
      "Processed 37000/105891 windows\n",
      "Processed 38000/105891 windows\n",
      "Processed 39000/105891 windows\n",
      "Processed 40000/105891 windows\n",
      "Processed 41000/105891 windows\n",
      "Processed 42000/105891 windows\n",
      "Processed 43000/105891 windows\n",
      "Processed 44000/105891 windows\n",
      "Processed 45000/105891 windows\n",
      "Processed 46000/105891 windows\n",
      "Processed 47000/105891 windows\n",
      "Processed 48000/105891 windows\n",
      "Processed 49000/105891 windows\n",
      "Processed 50000/105891 windows\n",
      "Processed 51000/105891 windows\n",
      "Processed 52000/105891 windows\n",
      "Processed 53000/105891 windows\n",
      "Processed 54000/105891 windows\n",
      "Processed 55000/105891 windows\n",
      "Processed 56000/105891 windows\n",
      "Processed 57000/105891 windows\n",
      "Processed 58000/105891 windows\n",
      "Processed 59000/105891 windows\n",
      "Processed 60000/105891 windows\n",
      "Processed 61000/105891 windows\n",
      "Processed 62000/105891 windows\n",
      "Processed 63000/105891 windows\n",
      "Processed 64000/105891 windows\n",
      "Processed 65000/105891 windows\n",
      "Processed 66000/105891 windows\n",
      "Processed 67000/105891 windows\n",
      "Processed 68000/105891 windows\n",
      "Processed 69000/105891 windows\n",
      "Processed 70000/105891 windows\n",
      "Processed 71000/105891 windows\n",
      "Processed 72000/105891 windows\n",
      "Processed 73000/105891 windows\n",
      "Processed 74000/105891 windows\n",
      "Processed 75000/105891 windows\n",
      "Processed 76000/105891 windows\n",
      "Processed 77000/105891 windows\n",
      "Processed 78000/105891 windows\n",
      "Processed 79000/105891 windows\n",
      "Processed 80000/105891 windows\n",
      "Processed 81000/105891 windows\n",
      "Processed 82000/105891 windows\n",
      "Processed 83000/105891 windows\n",
      "Processed 84000/105891 windows\n",
      "Processed 85000/105891 windows\n",
      "Processed 86000/105891 windows\n",
      "Processed 87000/105891 windows\n",
      "Processed 88000/105891 windows\n",
      "Processed 89000/105891 windows\n",
      "Processed 90000/105891 windows\n",
      "Processed 91000/105891 windows\n",
      "Processed 92000/105891 windows\n",
      "Processed 93000/105891 windows\n",
      "Processed 94000/105891 windows\n",
      "Processed 95000/105891 windows\n",
      "Processed 96000/105891 windows\n",
      "Processed 97000/105891 windows\n",
      "Processed 98000/105891 windows\n",
      "Processed 99000/105891 windows\n",
      "Processed 100000/105891 windows\n",
      "Processed 101000/105891 windows\n",
      "Processed 102000/105891 windows\n",
      "Processed 103000/105891 windows\n",
      "Processed 104000/105891 windows\n",
      "Processed 105000/105891 windows\n"
     ]
    }
   ],
   "source": [
    "X_features = create_feature_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd4c800-2a2a-4993-8cc1-1282b19d26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION\n",
      "Training Accuracy: 0.8590\n",
      "Test Accuracy: 0.8016\n",
      "Macro F1-Score: 0.6793\n",
      "Weighted F1-Score: 0.7817\n",
      "--------------------------------------------------\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N1       0.57      0.14      0.23      1765\n",
      "          N2       0.74      0.90      0.81      8011\n",
      "          N3       0.86      0.68      0.76       541\n",
      "           R       0.87      0.62      0.72      1680\n",
      "           W       0.86      0.89      0.87      9182\n",
      "\n",
      "    accuracy                           0.80     21179\n",
      "   macro avg       0.78      0.64      0.68     21179\n",
      "weighted avg       0.79      0.80      0.78     21179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_xgboost_model(X_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809564ed-3520-457b-aebd-1afc4139ebd2",
   "metadata": {},
   "source": [
    "#### Several key insights can be drawn from the classification report of the XGBoost model. For instance, I was able to see the class imbalance between the sleep stages with the N3 (deep sleep) having the least amount of data. With more research, I learned that N3 is also the shortest period of sleep stage during the night for an average person. Despite having fewer samples than N1, N3 achieved better results - this could be explained by deep sleep having more distinctive physiological patterns that are easier for the model to recognize. In contrast, N1 performed poorly with only 8% recall, likely because it's a transitional stage between wake and sleep, making the patterns less clear. From this baseline model, I will know how to approach the dataset for my main model.  \n",
    "\n",
    "#### My average accuracy score for XGBoost is somewhere around 75% to 81% depending on the random state. I will use this as a benchmark to see if my CNN model can beat it. However, this accuracy score as a baseline can be quite difficult to beat since XGBoost is a very powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8133d-923e-48d3-9e61-c5a51a55b1fd",
   "metadata": {},
   "source": [
    "## I will now begin building my 1D CNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb23934",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "338f5fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: torch.Size([50827, 8, 1920])\n",
      "Validation set: torch.Size([12707, 8, 1920])\n",
      "DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fix the data preparation section\n",
    "label_encoder = LabelEncoder() \n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Reformatting numpy arrays to fit PyTorch's preferred format \n",
    "X_pytorch = np.transpose(data, (0, 2, 1))\n",
    "\n",
    "# First split: train vs test (60% train, 40% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pytorch, y_encoded, \n",
    "    test_size=0.4, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Second split: split train into train + validation (48% train, 12% val, 40% test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.2,  \n",
    "    random_state=42, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# Reshape for StandardScaler\n",
    "n_train, channels, length = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(n_train, -1)\n",
    "X_val_reshaped = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train_scaled = X_train_scaled.reshape(n_train, channels, length)\n",
    "X_val_scaled = X_val_scaled.reshape(X_val.shape[0], channels, length)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test.shape[0], channels, length)\n",
    "\n",
    "# Convert to tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train_tensor = torch.from_numpy(X_train_scaled).float()\n",
    "X_val_tensor = torch.from_numpy(X_val_scaled).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "y_val_tensor = torch.from_numpy(y_val).long()\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train set: {X_train_tensor.shape}\")\n",
    "print(f\"Validation set: {X_val_tensor.shape}\")\n",
    "print(\"DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c59bd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepCNN(nn.Module): # drawing traits from nn.Module\n",
    "    def __init__(self):\n",
    "        super(SleepCNN, self).__init__() # setting up Pytorch infrastructure \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(8, 16, kernel_size=11, padding=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=7, padding=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        \n",
    "        # After pooling: 1920 -> 480 -> 120\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(32 * 120, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a77c54",
   "metadata": {},
   "source": [
    "### Intializing the environment and model and testing if everything works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61d2821f-dc45-41bf-a263-dae0239c586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8, 1920])\n",
      "Output shape: torch.Size([4, 5])\n",
      "Model parameters: 251189\n"
     ]
    }
   ],
   "source": [
    "model = SleepCNN()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Test with dummy data\n",
    "test_input = torch.randn(4, 8, 1920).to(device)\n",
    "output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdb314",
   "metadata": {},
   "source": [
    "## Setting up the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e44fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up training components\n",
    "criterion = nn.CrossEntropyLoss()  # Multi-class classification loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4268598",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c850dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Enable dropout and batch norm training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()        # Reset gradients\n",
    "            outputs = model(batch_X)     # Forward pass\n",
    "            loss = criterion(outputs, batch_y)  # Calculate loss\n",
    "            loss.backward()              # Backward pass\n",
    "            optimizer.step()             # Update weights\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Disable dropout, enable evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted classes\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "        print('-' * 60)\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d9bcc",
   "metadata": {},
   "source": [
    "### Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0c95153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CNN training...\n",
      "Epoch [1/20]\n",
      "Train Loss: 1.0818, Val Loss: 1.0300, Val Acc: 60.35%\n",
      "------------------------------------------------------------\n",
      "Epoch [2/20]\n",
      "Train Loss: 1.0102, Val Loss: 0.9923, Val Acc: 62.61%\n",
      "------------------------------------------------------------\n",
      "Epoch [3/20]\n",
      "Train Loss: 0.9692, Val Loss: 0.9598, Val Acc: 63.26%\n",
      "------------------------------------------------------------\n",
      "Epoch [4/20]\n",
      "Train Loss: 0.9420, Val Loss: 0.9497, Val Acc: 64.03%\n",
      "------------------------------------------------------------\n",
      "Epoch [5/20]\n",
      "Train Loss: 0.9179, Val Loss: 0.9183, Val Acc: 64.65%\n",
      "------------------------------------------------------------\n",
      "Epoch [6/20]\n",
      "Train Loss: 0.8966, Val Loss: 0.9220, Val Acc: 65.42%\n",
      "------------------------------------------------------------\n",
      "Epoch [7/20]\n",
      "Train Loss: 0.8715, Val Loss: 0.9169, Val Acc: 64.82%\n",
      "------------------------------------------------------------\n",
      "Epoch [8/20]\n",
      "Train Loss: 0.8547, Val Loss: 0.9016, Val Acc: 65.45%\n",
      "------------------------------------------------------------\n",
      "Epoch [9/20]\n",
      "Train Loss: 0.8384, Val Loss: 0.8886, Val Acc: 66.34%\n",
      "------------------------------------------------------------\n",
      "Epoch [10/20]\n",
      "Train Loss: 0.8199, Val Loss: 0.8870, Val Acc: 66.44%\n",
      "------------------------------------------------------------\n",
      "Epoch [11/20]\n",
      "Train Loss: 0.8074, Val Loss: 0.8676, Val Acc: 66.66%\n",
      "------------------------------------------------------------\n",
      "Epoch [12/20]\n",
      "Train Loss: 0.7957, Val Loss: 0.8644, Val Acc: 68.03%\n",
      "------------------------------------------------------------\n",
      "Epoch [13/20]\n",
      "Train Loss: 0.7792, Val Loss: 0.8608, Val Acc: 67.93%\n",
      "------------------------------------------------------------\n",
      "Epoch [14/20]\n",
      "Train Loss: 0.7702, Val Loss: 0.8606, Val Acc: 67.58%\n",
      "------------------------------------------------------------\n",
      "Epoch [15/20]\n",
      "Train Loss: 0.7577, Val Loss: 0.8594, Val Acc: 68.04%\n",
      "------------------------------------------------------------\n",
      "Epoch [16/20]\n",
      "Train Loss: 0.7493, Val Loss: 0.8612, Val Acc: 67.62%\n",
      "------------------------------------------------------------\n",
      "Epoch [17/20]\n",
      "Train Loss: 0.7381, Val Loss: 0.8694, Val Acc: 68.09%\n",
      "------------------------------------------------------------\n",
      "Epoch [18/20]\n",
      "Train Loss: 0.7273, Val Loss: 0.8526, Val Acc: 68.30%\n",
      "------------------------------------------------------------\n",
      "Epoch [19/20]\n",
      "Train Loss: 0.7195, Val Loss: 0.8672, Val Acc: 68.25%\n",
      "------------------------------------------------------------\n",
      "Epoch [20/20]\n",
      "Train Loss: 0.7126, Val Loss: 0.8529, Val Acc: 68.49%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Final Validation Accuracy: 68.49%\n",
      "XGBoost Baseline: 80.00%\n",
      "CNN needs improvement to beat baseline.\n"
     ]
    }
   ],
   "source": [
    "# Train your model\n",
    "print(\"Starting CNN training...\")\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=20\n",
    ")\n",
    "\n",
    "# Check final validation accuracy\n",
    "final_val_acc = val_accuracies[-1]\n",
    "print(f\"\\nFinal Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "print(f\"XGBoost Baseline: 80.00%\")\n",
    "if final_val_acc > 80:\n",
    "    print(\"CNN beats XGBoost baseline!\")\n",
    "else:\n",
    "    print(\"CNN needs improvement to beat baseline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca804d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleep_study (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
